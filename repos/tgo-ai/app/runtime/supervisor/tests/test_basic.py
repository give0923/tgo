"""
Basic test to verify the coordination system v2 structure.

This script tests the basic imports and structure without requiring
external services.
"""

def test_imports():
    """Test that all core modules can be imported."""
    print("Testing imports...")
    
    try:
        # Test core models
        from .models.coordination import (
            CoordinationRequest, CoordinationResponse, QueryAnalysisResult,
            SubQuestion, ExecutionPlan, WorkflowType
        )
        print("âœ“ Coordination models imported successfully")
        
        from .models.execution import (
            WorkflowPattern, WorkflowPlan, ExecutionResult, AgentExecution
        )
        print("âœ“ Execution models imported successfully")
        
        from .models.results import (
            ConsolidationResult, ConsolidationStrategy, ConflictResolution
        )
        print("âœ“ Result models imported successfully")
        
        # Test exceptions
        from .exceptions.coordination import (
            CoordinationError, QueryAnalysisError, WorkflowPlanningError,
            ExecutionError, ConsolidationError, ValidationError
        )
        print("âœ“ Exception classes imported successfully")
        
        # Test configuration
        from .config.settings import (
            CoordinationConfig, QueryAnalysisConfig, WorkflowPlanningConfig,
            ExecutionConfig, ConsolidationConfig
        )
        print("âœ“ Configuration classes imported successfully")
        
        # Test interfaces
        from .interfaces.query_analyzer import IQueryAnalyzer
        from .interfaces.workflow_planner import IWorkflowPlanner
        from .interfaces.execution_manager import IExecutionManager
        from .interfaces.result_consolidator import IResultConsolidator
        print("âœ“ Interface classes imported successfully")
        
        # Test utilities
        from .utils.validation import validate_query_analysis_result
        from .utils.prompts import build_query_analysis_prompt
        from .utils.logging import get_coordination_logger
        from .utils.metrics import CoordinationMetrics
        print("âœ“ Utility functions imported successfully")
        
        print("\nâœ… All imports successful!")
        return True
        
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        return False
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        return False


def test_model_creation():
    """Test that models can be created and validated."""
    print("\nTesting model creation...")
    
    try:
        from .models.coordination import SubQuestion, ExecutionPlan, QueryAnalysisResult
        
        # Test SubQuestion creation
        sub_question = SubQuestion(
            id="sq_1",
            question="Test question",
            assigned_agent_id="agent-123"
        )
        print("âœ“ SubQuestion created successfully")
        
        # Test ExecutionPlan creation
        execution_plan = ExecutionPlan(
            dependencies=[],
            parallel_groups=[["agent-123"]],
            estimated_time=30.0
        )
        print("âœ“ ExecutionPlan created successfully")
        
        # Test QueryAnalysisResult creation
        analysis_result = QueryAnalysisResult(
            selected_agent_ids=["agent-123"],
            selection_reasoning="Test reasoning",
            workflow="parallel",
            workflow_reasoning="Test workflow reasoning",
            confidence_score=0.9,
            is_complex=True,
            sub_questions=[sub_question],
            execution_plan=execution_plan
        )
        print("âœ“ QueryAnalysisResult created successfully")
        
        # Test workflow type validation
        assert analysis_result.workflow_type.value == "parallel"
        print("âœ“ Workflow type validation works")
        
        print("\nâœ… All model creation tests passed!")
        return True
        
    except Exception as e:
        print(f"âŒ Model creation error: {e}")
        return False


def test_configuration():
    """Test configuration system."""
    print("\nTesting configuration...")
    
    try:
        from .config.settings import CoordinationConfig, get_coordination_config
        
        # Test default configuration
        config = CoordinationConfig()
        print("âœ“ Default configuration created")
        
        # Test configuration validation
        assert config.query_analysis.temperature >= 0.0
        assert config.query_analysis.temperature <= 2.0
        assert config.execution.default_timeout > 0
        assert config.consolidation.confidence_threshold >= 0.0
        assert config.consolidation.confidence_threshold <= 1.0
        print("âœ“ Configuration validation works")
        
        # Test environment configuration loading
        env_config = get_coordination_config()
        print("âœ“ Environment configuration loaded")
        
        print("\nâœ… All configuration tests passed!")
        return True
        
    except Exception as e:
        print(f"âŒ Configuration error: {e}")
        return False


def test_validation():
    """Test validation utilities."""
    print("\nTesting validation...")
    
    try:
        from .utils.validation import validate_query_analysis_result, validate_agent_id
        from .models.coordination import QueryAnalysisResult, SubQuestion, ExecutionPlan
        from app.models.internal import CoordinationContext, Team, Agent
        
        # Create test data
        agents = [
            Agent(id="agent-1", name="Test Agent", description="Test", capabilities=[], is_active=True)
        ]
        team = Team(id="team-1", name="Test Team", instruction="Test", agents=agents)
        context = CoordinationContext(
            team=team, message="Test message", execution_strategy="test",
            max_agents=1, timeout=300, require_consensus=False
        )
        
        # Create valid analysis result
        analysis_result = QueryAnalysisResult(
            selected_agent_ids=["agent-1"],
            selection_reasoning="Test reasoning",
            workflow="single",
            workflow_reasoning="Single agent sufficient",
            confidence_score=0.8,
            is_complex=False,
            sub_questions=[],
            execution_plan=ExecutionPlan()
        )
        
        # Test validation
        errors = validate_query_analysis_result(analysis_result, context)
        assert len(errors) == 0, f"Validation errors: {errors}"
        print("âœ“ Query analysis result validation works")
        
        # Test agent ID validation
        assert validate_agent_id("550e8400-e29b-41d4-a716-446655440000") == True
        assert validate_agent_id("invalid-id") == False
        print("âœ“ Agent ID validation works")
        
        print("\nâœ… All validation tests passed!")
        return True
        
    except Exception as e:
        print(f"âŒ Validation error: {e}")
        return False


def test_metrics():
    """Test metrics system."""
    print("\nTesting metrics...")
    
    try:
        from .utils.metrics import CoordinationMetrics, get_global_metrics
        
        # Test metrics creation
        metrics = CoordinationMetrics()
        print("âœ“ Metrics instance created")
        
        # Test metrics recording
        metrics.record_request(True)
        metrics.record_query_analysis(1.5, True)
        metrics.record_workflow_execution(2.0, True, "parallel")
        metrics.record_consolidation(0.8, True)
        print("âœ“ Metrics recording works")
        
        # Test metrics calculation
        assert metrics.success_rate == 1.0
        assert metrics.query_analysis_avg_time == 1.5
        assert metrics.workflow_execution_avg_time == 2.0
        print("âœ“ Metrics calculation works")
        
        # Test global metrics
        global_metrics = get_global_metrics()
        assert global_metrics is not None
        print("âœ“ Global metrics accessible")
        
        print("\nâœ… All metrics tests passed!")
        return True
        
    except Exception as e:
        print(f"âŒ Metrics error: {e}")
        return False


def main():
    """Run all basic tests."""
    print("Coordination System v2 - Basic Structure Test")
    print("=" * 50)
    
    tests = [
        test_imports,
        test_model_creation,
        test_configuration,
        test_validation,
        test_metrics
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"âŒ Test {test.__name__} failed with exception: {e}")
    
    print("\n" + "=" * 50)
    print(f"Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("ðŸŽ‰ All basic structure tests passed!")
        print("\nThe coordination system v2 is properly structured and ready for use.")
        print("\nKey features verified:")
        print("âœ“ Clean architecture with proper separation of concerns")
        print("âœ“ Type-safe data models with validation")
        print("âœ“ Comprehensive configuration system")
        print("âœ“ Robust error handling and validation")
        print("âœ“ Performance monitoring and metrics")
        print("âœ“ Interface-based design for testability")
    else:
        print("âŒ Some tests failed. Please check the errors above.")
    
    return passed == total


if __name__ == "__main__":
    main()
